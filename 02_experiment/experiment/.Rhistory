# Working dir
basedir <- system("git rev-parse --show-toplevel", intern=TRUE)
scriptdir <- file.path(basedir, 'development', 'replication', 'katagiri2019', 'replication_materials')
setwd(file.path(scriptdir, 'data'))
knitr::opts_knit$set(root.dir = file.path(scriptdir, 'data'))
# 3 collections
collections <- c("wh", "dos", "fbis")
data_files <- c( # All the unique data files
"All", # ID and date for documents
"DTMAll", # Document-term matrix for all docs
"DTM", # Randomly selected sample
"Threat", # Hand-coded sample
"AllPred" # Labels
)
# Check exists
for (col in collections){
for (dtf in data_files) {
stopifnot(file.exists(paste0(col, dtf, '.csv')))}}
getwd()
k <- 1
collection = collections[k]
print(paste("Producing", toupper(collection), "data...", sep=" "))
### Set up the data
# ID and date for all documents
datAll = read.csv(paste(collection, "All.csv", sep=""))
getwd()
# Working dir
basedir <- system("git rev-parse --show-toplevel", intern=TRUE)
scriptdir <- file.path(basedir, 'development', 'replication', 'katagiri2019', 'replication_materials')
setwd(file.path(scriptdir, 'data'))
# knitr::opts_knit$set(root.dir = file.path(scriptdir, 'data'))
# 3 collections
collections <- c("wh", "dos", "fbis")
data_files <- c( # All the unique data files
"All", # ID and date for documents
"DTMAll", # Document-term matrix for all docs
"DTM", # Randomly selected sample
"Threat", # Hand-coded sample
"AllPred" # Labels
)
# Check exists
for (col in collections){
for (dtf in data_files) {
stopifnot(file.exists(paste0(col, dtf, '.csv')))}}
setwd(file.path(scriptdir, 'data'))
k <- 1
collection = collections[k]
print(paste("Producing", toupper(collection), "data...", sep=" "))
### Set up the data
# ID and date for all documents
datAll = read.csv(paste(collection, "All.csv", sep=""))
datAll = datAll[,-which(names(datAll)=="X")] # Filter out `index`
datAll$date = as.Date(as.character(datAll$date), "%m/%d/%Y")
# Document-term matrix for all documents
dfAll = read.csv(paste(collection, "DTMAll.csv", sep=""))
dfAll = dfAll[,-which(names(dfAll)=="X")]
# Document-term matrix for randomly selected sample
df = read.csv(paste(collection, "DTM.csv", sep=""))
df = df[,-which(names(df)=="X")]
# Manual coding of resolve for randomly selected sample
tf = read.csv(paste(collection, "Threat.csv", sep=""))
# Checks:
all(colnames(df) %in% colnames(dfAll)) # TRUE
sum(colnames(dfAll) %in% colnames(df)) # 394 / 1410
wTrain = caret::createDataPartition(y = tf$resolve,
p = 0.75,
list = FALSE)
wTrainSet = data.frame(df, Resolve=tf$resolve)[wTrain,]
wTestSet = data.frame(df, Resolve=tf$resolve)[-wTrain,]
wTrainSet$Resolve = (ifelse(wTrainSet$Resolve=="None", "No", "Yes"))
wTrainSet$Resolve = factor(wTrainSet$Resolve)
wTestSet$Resolve = (ifelse(wTestSet$Resolve=="None", "No", "Yes"))
wTestSet$Resolve = factor(wTestSet$Resolve)
Xtrain = wTrainSet[,1:(ncol(wTrainSet)-1)]
Xtest = wTestSet[,1:(ncol(wTrainSet)-1)]
Ytrain = factor(as.numeric(wTrainSet[,ncol(wTrainSet)])-1)
Ytest = factor(as.numeric(wTestSet[,ncol(wTestSet)])-1)
setwd(file.path(scriptdir, 'data'))
nmin = sum(wTrainSet$Resolve=="Yes") # N positives
# Fit model
rfFit = randomForest::randomForest(Xtrain, Ytrain, data=wTrainSet, ntree = 2000,
strata = Ytrain, mtry = 40,
sampsize = rep(nmin, 2), keep.inbag=TRUE)
rfProbs = 1-predict(rfFit, type = "prob", newdata = wTestSet)[,1] # Predictions
rfClasses = ifelse(rfProbs >= 0.5, 1, 0) # Dichotomize
cm = caret::confusionMatrix(data = factor(rfClasses), factor(as.numeric(wTestSet$Resolve)-1)) # Confusion matrix
# Extrapolate usign infintesimal jackknife
ij = randomForestCI::randomForestInfJack(rfFit, Xtest, calibrate=T)
ij2 = randomForestCI::randomForestInfJack(rfFit, dfAll, calibrate=TRUE)
# Add back into data
datAll$predMean = ij2[,1]
datAll$predVar = ij2[,2]
# Check that is same as AllPred
max(read.csv(paste0(collection, 'AllPred.csv'))$predMean-datAll$predMean) # 0.556?
# First, combine segments back into documents
doc.id = unique(datAll$doc.id)
length(doc.id)
max(table(datAll$doc.id))
date = datAll$date[datAll$doc.id %in% doc.id]
threatAvg = day = NA
for (i in 1:length(doc.id)) {
docSegments = datAll[datAll$doc.id==doc.id[i],]
nSegments = nrow(docSegments)
avg = sum(docSegments$predMean)/nSegments
threatAvg[i] = ifelse(avg >= 0.5, 1, 0)
day[i] = as.character(docSegments$date[1])
}
threats = data.frame(doc.id, date=day, threatAvg)
threats$date = as.Date(threats$date)
# Weekly date sequence
dinit = as.Date("01-01-1958", "%m-%d-%Y")
dfinal = as.Date("11-26-1963", "%m-%d-%Y")
weeks = seq(from = dinit, to = dfinal, by='weeks')
# Aggregate documents by week
wsumAvg = ndoc = NA
for (i in 1:(length(weeks)-1)) {
oneweek = threats[(threats$date >= weeks[i] & threats$date < weeks[i+1]),]
oneweek = oneweek[!is.na(oneweek$date),]
wsumAvg[i] = sum(oneweek$threatAvg, na.rm=T)
ndoc[i] = nrow(oneweek)
}
weeklyPred = data.frame(week=weeks[-length(weeks)], wsumAvg, ndoc)
datAll$week <- NA
for (i in 1:length(weeks)) {
threats$week[c(threats$date >= weeks[i] & threats$date < weeks[i+1])] <- i
}
doc_mod <- lm(predMean ~ doc.id, data=datAll)
doc_mod <- lm(predMean ~ doc.id, data=datAll)
doc_mod <- lm(predMean ~ doc.id, data=da
doc_mod <- lm(predMean ~ doc.id, data=da
caret::trainControl(method-'cv', number=5)
caret::trainControl(method='cv', number=5)
doc_mod
caret::createFolds(tf$resolve, k=10, list=TRUE)
caret::createFolds(tf$resolve, k=5, list=TRUE)
dfAll
dfAll[,colnames(df)]
apply(
X=dfAll[,colnames(df)]
FUN=function(row){all(row==df[1, ])}
apply(
X=dfAll[,colnames(df)],
FUN=function(row){all(row==df[1, ])},
MARGIN=1
)
df[1, ]
df[1, ] %in% dfAll[,colnames(df)]
dfAll[,colnames(df)]
as.list(dfAll[,colnames(df)])
as.list.data.frame
split(dfAll[,colnames(df)], seq(nrow(dfAll)))
list_df <- split(dfAll[,colnames(df)], seq(nrow(dfAll)))
list_df <- split(dfAll[,colnames(df)], seq(nrow(dfAll)))
list_df[[1]]
as.numeric(lapply[[1]])
as.vector(lapply[[1]]
as.vector(lapply[[1]])
as.numeric(list_df[[1]])
lapply(list_df, as.numeric)[[1]]
list_df <- lapply(list_df, as.numeric)[[1]]
list_df[[1]]==df[1,]
all(list_df[[1]]==df[1,])
all(list_df[[1]]==as.numeric(df[1,]))
tmp <- lapply(
list_df,
function(row) {all(row==as.numeric(df[1,]))}
)
any(tmp)
tmp
unlist(tmp)
which(unlist(tmp))
list_df[[1]]
list_df[[2]]
list_df <- split(dfAll[,colnames(df)], seq(nrow(dfAll)))
list_df <- lapply(list_df, as.numeric)[[1]]
list_df[[1]]
list_df <- split(dfAll[,colnames(df)], seq(nrow(dfAll)))
list_df <- lapply(list_df, as.numeric)
list_df[[1]]
as.numeric(df[1,])
list_df[[1]] == as.numeric(df[1,])
tmp <- lapply(
list_df,
function(row) {all(row==as.numeric(df[1,]))}
)
tmp[[1]]
which(unlist(tmp))
which(unlist(tmp))
unlist(tmp)
any(unlist(tmp))
length(tmp)
tmp <- apply(
X=dfAll[,colnames(df)],
FUN=function(row){all(row==df[1, ])},
MARGIN=1
)
tmp <- apply(
X=dfAll[,colnames(df)],
FUN=function(row){all(row==df[1, ])},
MARGIN=1
)
as.
tmp
length(tmp)
any(tmp)
which(tmp)
colnames(df)
big <- as.matrix(dfAll)
big
big[1, :]
big[1, ]
big <- as.matrix(dfAll[, colnames(df)])
small <- as.matrix(df)
cos_sim <- function(small, big) {
small%*%t(big)/sqrt(tcrossprod(rowSums(small^2), rowSums(big^2)))
}
cos_sim
cos_sim(small, big)[2,1]
lsa::cosine(big[1,], small[2,])
lsa::cosine(big[1,], small[2,])
all.equal(lsa::cosine(t(small)), as.matrix(cos_sim(small,small)), check.attributes=FALSE)
apply(
X=big[1:5, ],
FUN=print,
margin=1
)
apply(
X=big[1:5, ],
FUN=print,
MARGIN=1
)
apply(
X=big[1:3, 1:10],
FUN=print,
MARGIN=1
)
out <- apply(
X=small,
FUN=function(small_row) {
apply(
X=big,
FUN=function(big_row){cos_sim(small_row, big_row)}
MARGIN=1
out <- apply(
X=small,
FUN=function(small_row) {
apply(
X=big,
FUN=function(big_row){cos_sim(small_row, big_row)},
MARGIN=1)
},
MARGIN=1)
out <- cos_sim(small, big)
dim(out)
dim(small)
sapply(max, out)
sapply(out, max)
apply(X=out, FUN=max, MARGIN=1)
max(out[1, ])
max(out[1, ], na.rm=TRUE)
apply(X=out, FUN=function(x){max(x, na.rm=TRUE)}, MARGIN=1)
min(max(out[1, ], na.rm=TRUE))
min(max(out[1, ], na.rm=TRUE))
min(apply(X=out, FUN=function(x){max(x, na.rm=TRUE)}, MARGIN=1))
which(apply(X=out, FUN=function(x){max(x, na.rm=TRUE)}, MARGIN=1)<1) # Check the one that isn't right
df[:,602]
df[602,]
df
df
setwd(file.path(scriptdir, 'data'))
k <- 1
collection = collections[k]
print(paste("Producing", toupper(collection), "data...", sep=" "))
### Set up the data
# ID and date for all documents
datAll = read.csv(paste(collection, "All.csv", sep=""))
datAll = datAll[,-which(names(datAll)=="X")] # Filter out `index`
datAll$date = as.Date(as.character(datAll$date), "%m/%d/%Y")
# Document-term matrix for all documents
dfAll = read.csv(paste(collection, "DTMAll.csv", sep=""))
dfAll = dfAll[,-which(names(dfAll)=="X")]
# Document-term matrix for randomly selected sample
df = read.csv(paste(collection, "DTM.csv", sep=""))
df = df[,-which(names(df)=="X")]
# Manual coding of resolve for randomly selected sample
tf = read.csv(paste(collection, "Threat.csv", sep=""))
# Checks:
all(colnames(df) %in% colnames(dfAll)) # TRUE
sum(colnames(dfAll) %in% colnames(df)) # 394 / 1410
big <- as.matrix(dfAll[, colnames(df)])
small <- as.matrix(df)
df
df[602,]
small[602, ]
unlist(small[602, ])
as.numeric(small[602, ])
which.min(out[602, ])
big[36,] - small[602,]
lsa::cosine(big[36,], small[602,])
which.max(out[602, ])
big[6201,] - small[602,]
sum(abs(big[6201,] - small[602,]))
lsa::cosine(big[6201,], small[602,])
BASE_DIR <- '~/Dev/'
SCRIPT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation')
OUT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation/sim_res/run_001')
if (!dir.exists(OUT_DIR)){ dir.create(OUT_DIR) }
LIBS_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/function/')
BASE_DIR <- '~/Dev/'
SCRIPT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/simulation/run/')
LIBS_DIR <- path.file(BASE_DIR, 'debiasedproxies', 'development', 'libs', 'estimators')
LIBS_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'libs', 'estimators')
LIBS_DIR
BASE_DIR <- '~/Dev'
BASE_DIR <- '~/Dev/'
# SCRIPT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation')
# OUT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation/sim_res/run_001')
# if (!dir.exists(OUT_DIR)){ dir.create(OUT_DIR) }
SCRIPT_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'simulation', 'run')
LIBS_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'libs', 'estimators')
LIBS_DIR
SCRIPT_DIR
BASE_DIR <- '~/Dev'
# SCRIPT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation')
# OUT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation/sim_res/run_001')
# if (!dir.exists(OUT_DIR)){ dir.create(OUT_DIR) }
SCRIPT_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'simulation', 'run')
LIBS_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'libs', 'estimators')
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies')
SCRIPT_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'simulation', 'run')
LIBS_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'libs', 'estimators')
set.seed(100)
#########
# BASE_DIR <- '~/Dev/'
# SCRIPT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation')
# OUT_DIR <- paste0(BASE_DIR, 'debiasedproxies/development/estimators/code/simulation/sim_res/run_001')
# if (!dir.exists(OUT_DIR)){ dir.create(OUT_DIR) }
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies')
SCRIPT_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'simulation', 'run')
LIBS_DIR <- file.path(BASE_DIR, 'debiasedproxies', 'development', 'libs', 'estimators')
setwd(SCRIPT_DIR)
SCRIPT_DIR
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies')
SCRIPT_DIR <- file.path(BASE_DIR, 'development', 'simulation', 'run')
LIBS_DIR <- file.path(BASE_DIR, 'development', 'libs', 'estimators')
setwd(SCRIPT_DIR)
source(paste0(LIBS_DIR, "debias_logit.R"))
require(survey)
create_dummy_vars <- function(df, col_names) {
new_colnames_list <- c()
for (col_name in col_names) {
print(col_name)
# Convert column to a factor
df[[col_name]] <- factor(df[[col_name]])
# Create dummy variables using model.matrix
dummy_vars <- model.matrix( ~ 0 + df[[col_name]])
# Generate unique names for the new columns
num_levels <- nlevels(df[[col_name]])
col_suffixes <- seq_len(num_levels)
#col_prefix <- paste0("X", ncol(df) + 1, "_", col_name)
col_prefix <- paste0("X", "_", col_name)
new_colnames <- paste0(col_prefix, col_suffixes)
print(new_colnames)
colnames(dummy_vars) <- new_colnames
# Add the dummy variables to the data frame
df <- cbind(df, dummy_vars)
new_colnames_list <- c(new_colnames_list, new_colnames)
}
# Return the updated data frame
return(list(df, new_colnames_list))
}
data_file <- ifelse(DATASET=='easy',
"../../../datasets/final/cbp_easy_with_proxies.csv",
"../../../datasets/final/cbp_imbalanced_with_proxies.csv" )
DATASET='easy'
data_file <- ifelse(DATASET=='easy',
"../../../datasets/final/cbp_easy_with_proxies.csv",
"../../../datasets/final/cbp_imbalanced_with_proxies.csv" )
data_use <- read.csv(data_file)
PROBLEM_TYPE <- "logit" # "measure" or "logit"
# DATASET <- 'binary_conv_go_awry_pos-yes.csv' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
DATASET <- 'easy' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
# PROXY_TYPE <- c("all", "openai", "hf", "best", "text-davinci-003","flan-ul2") # options: "all", "best","hf", "openai","flan-t5-small", "flan-t5-base", "flan-t5-large", "flan-t5-xl","flan-t5-xxl","flan-ul2","text-ada-001","text-babbage-001","text-curie-001","text-davinci-001","text-davinci-002","text-davinci-003","chatgpt"
SIM_SIZE <- 500 # number of simulation
N_BOOTSTRAP <- 100 # number of bootstrap
OUT_FOLDER <- "run_001" # output folder
PROBLEM_TYPE <- "logit" # "measure" or "logit"
# DATASET <- 'binary_conv_go_awry_pos-yes.csv' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
DATASET <- 'easy' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
# PROXY_TYPE <- c("all", "openai", "hf", "best", "text-davinci-003","flan-ul2") # options: "all", "best","hf", "openai","flan-t5-small", "flan-t5-base", "flan-t5-large", "flan-t5-xl","flan-t5-xxl","flan-ul2","text-ada-001","text-babbage-001","text-curie-001","text-davinci-001","text-davinci-002","text-davinci-003","chatgpt"
SIM_SIZE <- 500 # number of simulation
N_BOOTSTRAP <- 100 # number of bootstrap
OUT_FOLDER <- "run_001" # output folder
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies', 'archive', 'neurips_replication_materials')
dir.exists(BASE_DIR)
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies', 'archive', 'neurips_replication_materials')
SCRIPT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/')
DATASET_DIR <- paste0(BASE_DIR, '01_data_and_preprocessing/measurement/binarized_datasets/')
setwd(SCRIPT_DIR)
SCRIPT_DIR
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies', 'archive', 'neurips_replication_materials/')
SCRIPT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/')
DATASET_DIR <- paste0(BASE_DIR, '01_data_and_preprocessing/measurement/binarized_datasets/')
OUT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/', OUT_FOLDER)
setwd(SCRIPT_DIR)
if (PROBLEM_TYPE == "measure") {
if (!grepl(".csv", DATASET)) {
stop("Invalid DATASET for PROBLEM_TYPE 'measure', you must input the full csv file name.")
}
R_VALS <- c(25, 50, 100, 150, 200) # Number of documents to read
source("experiment_measure.R")
} else if (PROBLEM_TYPE == "logit") {
if (!(DATASET %in% c("easy", "imbalanced"))) {
stop("Invalid DATASET for PROBLEM_TYPE 'logit', you must choose 'easy' or 'imbalanced'")
}
R_VALS <- c(50, 100, 250, 500, 1000) # Number of documents to read
source("experiment_logit.R")
} else {
stop("Invalid PROBLEM_TYPE")
}
job_idx <- as.numeric(Sys.getenv("SLURM_ARRAY_TASK_ID"))
n_jobs <- as.numeric(Sys.getenv("SLURM_ARRAY_TASK_COUNT"))
n_sims <- total_sim_size
n_tasks <-  floor(n_sims/n_jobs)
remainder <- n_sims%%n_jobs
jobs <- (n_tasks*(job_idx-1)+1):(n_tasks*(job_idx))
profvis::profvis({
# DEBUG: Profiling
simulation_loop(s=123)
})
data_use
simulation_loop(s=123)
debugger
debugger()
debugonce(simulation_loop(s=123))
PROBLEM_TYPE <- "logit" # "measure" or "logit"
# DATASET <- 'binary_conv_go_awry_pos-yes.csv' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
DATASET <- 'easy' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
# PROXY_TYPE <- c("all", "openai", "hf", "best", "text-davinci-003","flan-ul2") # options: "all", "best","hf", "openai","flan-t5-small", "flan-t5-base", "flan-t5-large", "flan-t5-xl","flan-t5-xxl","flan-ul2","text-ada-001","text-babbage-001","text-curie-001","text-davinci-001","text-davinci-002","text-davinci-003","chatgpt"
SIM_SIZE <- 500 # number of simulation
N_BOOTSTRAP <- 100 # number of bootstrap
OUT_FOLDER <- "run_001" # output folder
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies', 'archive', 'neurips_replication_materials/')
SCRIPT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/')
DATASET_DIR <- paste0(BASE_DIR, '01_data_and_preprocessing/measurement/binarized_datasets/')
OUT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/', OUT_FOLDER)
setwd(SCRIPT_DIR)
if (PROBLEM_TYPE == "measure") {
if (!grepl(".csv", DATASET)) {
stop("Invalid DATASET for PROBLEM_TYPE 'measure', you must input the full csv file name.")
}
R_VALS <- c(25, 50, 100, 150, 200) # Number of documents to read
source("experiment_measure.R")
} else if (PROBLEM_TYPE == "logit") {
if (!(DATASET %in% c("easy", "imbalanced"))) {
stop("Invalid DATASET for PROBLEM_TYPE 'logit', you must choose 'easy' or 'imbalanced'")
}
R_VALS <- c(50, 100, 250, 500, 1000) # Number of documents to read
source("experiment_logit.R")
} else {
stop("Invalid PROBLEM_TYPE")
}
job_idx <- as.numeric(Sys.getenv("SLURM_ARRAY_TASK_ID"))
n_jobs <- as.numeric(Sys.getenv("SLURM_ARRAY_TASK_COUNT"))
# Map from job_idx to sim_num
n_sims <- total_sim_size
n_tasks <-  floor(n_sims/n_jobs)
remainder <- n_sims%%n_jobs
jobs <- (n_tasks*(job_idx-1)+1):(n_tasks*(job_idx))
profvis::profvis({
# DEBUG: Profiling
simulation_loop(s=123)
})
profvis::profvis({
# DEBUG: Profiling
simulation_loop(s=123)
})
# PROBLEM_TYPE <- "measure" # "measure" or "logit"
PROBLEM_TYPE <- "logit" # "measure" or "logit"
# DATASET <- 'binary_conv_go_awry_pos-yes.csv' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
DATASET <- 'easy' # NOTE: For measure case, input the full file name to DATASET here; For logit case, the DATASET should be "easy" or "imbalanced" (easy represent balanced case).
# PROXY_TYPE <- c("all", "openai", "hf", "best", "text-davinci-003","flan-ul2") # options: "all", "best","hf", "openai","flan-t5-small", "flan-t5-base", "flan-t5-large", "flan-t5-xl","flan-t5-xxl","flan-ul2","text-ada-001","text-babbage-001","text-curie-001","text-davinci-001","text-davinci-002","text-davinci-003","chatgpt"
SIM_SIZE <- 500 # number of simulation
N_BOOTSTRAP <- 100 # number of bootstrap
OUT_FOLDER <- "run_001" # output folder
# Directories
BASE_DIR = '<PATH>/<TO>/<REPO>/replication/' # base dir, where the project code located
BASE_DIR <- file.path('~', 'Dev', 'debiasedproxies', 'archive', 'neurips_replication_materials/')
SCRIPT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/')
DATASET_DIR <- paste0(BASE_DIR, '01_data_and_preprocessing/measurement/binarized_datasets/')
OUT_DIR <- paste0(BASE_DIR, '02_experiment/experiment/', OUT_FOLDER)
## Source
setwd(SCRIPT_DIR)
if (PROBLEM_TYPE == "measure") {
if (!grepl(".csv", DATASET)) {
stop("Invalid DATASET for PROBLEM_TYPE 'measure', you must input the full csv file name.")
}
R_VALS <- c(25, 50, 100, 150, 200) # Number of documents to read
source("experiment_measure.R")
} else if (PROBLEM_TYPE == "logit") {
if (!(DATASET %in% c("easy", "imbalanced"))) {
stop("Invalid DATASET for PROBLEM_TYPE 'logit', you must choose 'easy' or 'imbalanced'")
}
R_VALS <- c(50, 100, 250, 500, 1000) # Number of documents to read
source("experiment_logit.R")
} else {
stop("Invalid PROBLEM_TYPE")
}
profvis::profvis({
# DEBUG: Profiling
simulation_loop(s=123)
})
